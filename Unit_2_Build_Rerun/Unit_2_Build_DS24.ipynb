{
 "cells": [
  {
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 2, Sprint 3, Module 2*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Wrangle ML datasets\n",
    "\n",
    "- [ ] Continue to clean and explore your data. \n",
    "- [ ] For the evaluation metric you chose, what score would you get just by guessing?\n",
    "- [ ] Can you make a fast, first model that beats guessing?\n",
    "\n",
    "**We recommend that you use your portfolio project dataset for all assignments this sprint.**\n",
    "\n",
    "**But if you aren't ready yet, or you want more practice, then use the New York City property sales dataset for today's assignment.** Follow the instructions below, to just keep a subset for the Tribeca neighborhood, and remove outliers or dirty data. [Here's a video walkthrough](https://youtu.be/pPWFw8UtBVg?t=584) you can refer to if you get stuck or want hints!\n",
    "\n",
    "- Data Source: [NYC OpenData: NYC Citywide Rolling Calendar Sales](https://data.cityofnewyork.us/dataset/NYC-Citywide-Rolling-Calendar-Sales/usep-8jbt)\n",
    "- Glossary: [NYC Department of Finance: Rolling Sales Data](https://www1.nyc.gov/site/finance/taxes/property-rolling-sales-data.page)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(427, 11)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            corp_profits  exports_goods_svs  net_exports       gdp  \\\n",
       "1985-01-01       204.768            306.010      -91.298  7824.247   \n",
       "1985-02-01       204.768            306.010      -91.298  7824.247   \n",
       "1985-03-01       204.768            306.010      -91.298  7824.247   \n",
       "1985-04-01       205.850            304.126     -114.445  7893.136   \n",
       "1985-05-01       205.850            304.126     -114.445  7893.136   \n",
       "1985-06-01       205.850            304.126     -114.445  7893.136   \n",
       "1985-07-01       209.890            297.273     -116.895  8013.674   \n",
       "1985-08-01       209.890            297.273     -116.895  8013.674   \n",
       "1985-09-01       209.890            297.273     -116.895  8013.674   \n",
       "1985-10-01       211.649            305.433     -133.434  8073.239   \n",
       "\n",
       "            10yr_treasury    cpi  industrial_prod  unemployment_rate  \\\n",
       "1985-01-01      11.384286  105.7          56.1398                7.3   \n",
       "1985-02-01      11.508889  106.3          56.3323                7.2   \n",
       "1985-03-01      11.855238  106.8          56.4232                7.2   \n",
       "1985-04-01      11.434762  107.0          56.2693                7.3   \n",
       "1985-05-01      10.846818  107.2          56.3488                7.2   \n",
       "1985-06-01      10.156000  107.5          56.3901                7.4   \n",
       "1985-07-01      10.306818  107.7          56.0234                7.4   \n",
       "1985-08-01      10.331818  107.9          56.2555                7.1   \n",
       "1985-09-01      10.373158  108.1          56.4983                7.1   \n",
       "1985-10-01      10.236818  108.5          56.2648                7.1   \n",
       "\n",
       "            initial_claims  housing_starts  sp_ahead_pos_neg  \n",
       "1985-01-01        370750.0          1711.0              True  \n",
       "1985-02-01        391750.0          1632.0             False  \n",
       "1985-03-01        380800.0          1800.0             False  \n",
       "1985-04-01        398250.0          1821.0              True  \n",
       "1985-05-01        392750.0          1680.0              True  \n",
       "1985-06-01        392400.0          1676.0             False  \n",
       "1985-07-01        378250.0          1684.0             False  \n",
       "1985-08-01        404400.0          1743.0             False  \n",
       "1985-09-01        408500.0          1676.0              True  \n",
       "1985-10-01        405750.0          1834.0              True  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>corp_profits</th>\n      <th>exports_goods_svs</th>\n      <th>net_exports</th>\n      <th>gdp</th>\n      <th>10yr_treasury</th>\n      <th>cpi</th>\n      <th>industrial_prod</th>\n      <th>unemployment_rate</th>\n      <th>initial_claims</th>\n      <th>housing_starts</th>\n      <th>sp_ahead_pos_neg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1985-01-01</th>\n      <td>204.768</td>\n      <td>306.010</td>\n      <td>-91.298</td>\n      <td>7824.247</td>\n      <td>11.384286</td>\n      <td>105.7</td>\n      <td>56.1398</td>\n      <td>7.3</td>\n      <td>370750.0</td>\n      <td>1711.0</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1985-02-01</th>\n      <td>204.768</td>\n      <td>306.010</td>\n      <td>-91.298</td>\n      <td>7824.247</td>\n      <td>11.508889</td>\n      <td>106.3</td>\n      <td>56.3323</td>\n      <td>7.2</td>\n      <td>391750.0</td>\n      <td>1632.0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1985-03-01</th>\n      <td>204.768</td>\n      <td>306.010</td>\n      <td>-91.298</td>\n      <td>7824.247</td>\n      <td>11.855238</td>\n      <td>106.8</td>\n      <td>56.4232</td>\n      <td>7.2</td>\n      <td>380800.0</td>\n      <td>1800.0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1985-04-01</th>\n      <td>205.850</td>\n      <td>304.126</td>\n      <td>-114.445</td>\n      <td>7893.136</td>\n      <td>11.434762</td>\n      <td>107.0</td>\n      <td>56.2693</td>\n      <td>7.3</td>\n      <td>398250.0</td>\n      <td>1821.0</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1985-05-01</th>\n      <td>205.850</td>\n      <td>304.126</td>\n      <td>-114.445</td>\n      <td>7893.136</td>\n      <td>10.846818</td>\n      <td>107.2</td>\n      <td>56.3488</td>\n      <td>7.2</td>\n      <td>392750.0</td>\n      <td>1680.0</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1985-06-01</th>\n      <td>205.850</td>\n      <td>304.126</td>\n      <td>-114.445</td>\n      <td>7893.136</td>\n      <td>10.156000</td>\n      <td>107.5</td>\n      <td>56.3901</td>\n      <td>7.4</td>\n      <td>392400.0</td>\n      <td>1676.0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1985-07-01</th>\n      <td>209.890</td>\n      <td>297.273</td>\n      <td>-116.895</td>\n      <td>8013.674</td>\n      <td>10.306818</td>\n      <td>107.7</td>\n      <td>56.0234</td>\n      <td>7.4</td>\n      <td>378250.0</td>\n      <td>1684.0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1985-08-01</th>\n      <td>209.890</td>\n      <td>297.273</td>\n      <td>-116.895</td>\n      <td>8013.674</td>\n      <td>10.331818</td>\n      <td>107.9</td>\n      <td>56.2555</td>\n      <td>7.1</td>\n      <td>404400.0</td>\n      <td>1743.0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1985-09-01</th>\n      <td>209.890</td>\n      <td>297.273</td>\n      <td>-116.895</td>\n      <td>8013.674</td>\n      <td>10.373158</td>\n      <td>108.1</td>\n      <td>56.4983</td>\n      <td>7.1</td>\n      <td>408500.0</td>\n      <td>1676.0</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1985-10-01</th>\n      <td>211.649</td>\n      <td>305.433</td>\n      <td>-133.434</td>\n      <td>8073.239</td>\n      <td>10.236818</td>\n      <td>108.5</td>\n      <td>56.2648</td>\n      <td>7.1</td>\n      <td>405750.0</td>\n      <td>1834.0</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 137
    }
   ],
   "source": [
    "# Wrangle Data\n",
    "\n",
    "pd.set_option('display.max_rows', 150)\n",
    "\n",
    "def readIn(file, parse_d='DATE', idx='DATE'):\n",
    "    \"\"\" Opens .csv file, creates datetime index, and returns DataFrame\"\"\"\n",
    "\n",
    "    DATA_PATH = '../data/build_finance/'\n",
    "    df = pd.read_csv(DATA_PATH+file,\n",
    "                     parse_dates=[parse_d]).set_index(idx)\n",
    "    return df\n",
    "\n",
    "\n",
    "def manyToOne(files_m, files_q, file_w):\n",
    "    \"\"\"Accepts lists of .csv files and returns single DataFrame\"\"\"\n",
    "\n",
    "    # Takes monthly and quarterly files, has them read_in, parses their\n",
    "    # dates, and return DataFrames held in lists\n",
    "    frames_m = [readIn(file) for file in files_m]\n",
    "    frames_q = [readIn(file) for file in files_q]\n",
    "\n",
    "    # Reads in SP500 data, indicates columns to use, and capitalizes 'DATE'\n",
    "    # for consistency with other DataFrames held in frames_m and frames_q\n",
    "    DATA_PATH = '../data/build_finance/'\n",
    "    sp = pd.read_csv(DATA_PATH+file_w, usecols=['Date', 'Close'],\n",
    "                     parse_dates=['Date']).set_index('Date')\n",
    "    sp.rename(columns={'Date': 'DATE', 'Close': 'SP500_CLOSE'}, inplace=True)\n",
    "\n",
    "    # Concatenate DataFrames held in frames_m and frames_q\n",
    "    concat_m = pd.concat(frames_m, axis=1)\n",
    "    concat_q = pd.concat(frames_q, axis=1)\n",
    "\n",
    "    # Final concatenation of all DataFrames (monthly data, quarterly data \n",
    "    # and SP500 data)\n",
    "    last = pd.concat([concat_m, concat_q, sp], axis=1)\n",
    "\n",
    "    # SP500 data is only available from 1/1/1985\n",
    "    # Mask out dates prior to January 1st, 1985\n",
    "    mask = last.index >= '1985-01-01'\n",
    "    df_final = last[mask]\n",
    "    df_final.shape\n",
    "\n",
    "    # Return DataFrame\n",
    "    return df_final\n",
    "\n",
    "\n",
    "def wrangle(files_m, files_q, file_w):\n",
    "    # Pass .csv files and have a single DataFrame returned\n",
    "    df = manyToOne(files_m, files_q, file_w)\n",
    "\n",
    "    # Rename columns\n",
    "    df.columns = ['cpi', '10yr_treasury', 'housing_starts', \n",
    "                  'industrial_prod', 'initial_claims', 'unemployment_rate', \n",
    "                  'corp_profits', 'exports_goods_svs', 'gdp', 'net_exports',\n",
    "                  'sp500_close']\n",
    "\n",
    "    # Reorganize columns\n",
    "    cols_reorder = ['corp_profits', 'exports_goods_svs', 'net_exports', \n",
    "                    'gdp', '10yr_treasury', 'cpi', 'industrial_prod', \n",
    "                    'unemployment_rate', 'initial_claims', 'housing_starts', \n",
    "                    'sp500_close']\n",
    "\n",
    "    df = df.reindex(columns=cols_reorder)\n",
    "\n",
    "    # Create a target feature, month-ahead-return: positive/negative - \n",
    "    # Binary Classification\n",
    "    df['sp_ahead_pos_neg'] = (df['sp500_close'].shift(-1) - df['sp500_close']) > 0\n",
    "\n",
    "    # Drop leaky feature\n",
    "    df.drop(columns='sp500_close', inplace=True)\n",
    "\n",
    "    # Forward fill the quarterly data\n",
    "    df.ffill(inplace=True)\n",
    "\n",
    "    # Create new feature the shows the montly change in monthly initial unemployment \n",
    "    # claims\n",
    "    # df['change_initial_claims'] = df['initial_claims'] / df['initial_claims'].shift(+1)\n",
    "\n",
    "    # # Drop 'initial_claims'\n",
    "    # df.drop(columns='initial_claims')\n",
    "\n",
    "    # Return wrangled DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "file_w = '^GSPC_m.csv'\n",
    "\n",
    "files_m = ['CPI.csv',\n",
    "           'DGS10.csv',\n",
    "           'HOUST.csv',\n",
    "           'INDPRO.csv',\n",
    "           'INITCLMS.csv',\n",
    "           'UNRATE.csv']\n",
    "\n",
    "files_q = ['CP.csv',\n",
    "           'EXPGS.csv',\n",
    "           'GDP.csv',\n",
    "           'NETEXP.csv',]\n",
    "\n",
    "\n",
    "#df = manyToOne(files_m, files_q, file_w)\n",
    "\n",
    "df = wrangle(files_m, files_q, file_w)\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 427 entries, 1985-01-01 to 2020-07-01\nFreq: MS\nData columns (total 11 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   corp_profits       427 non-null    float64\n 1   exports_goods_svs  427 non-null    float64\n 2   net_exports        427 non-null    float64\n 3   gdp                427 non-null    float64\n 4   10yr_treasury      427 non-null    float64\n 5   cpi                427 non-null    float64\n 6   industrial_prod    427 non-null    float64\n 7   unemployment_rate  427 non-null    float64\n 8   initial_claims     427 non-null    float64\n 9   housing_starts     427 non-null    float64\n 10  sp_ahead_pos_neg   427 non-null    bool   \ndtypes: bool(1), float64(10)\nmemory usage: 53.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(427, 10) (427,)\n"
     ]
    }
   ],
   "source": [
    "# Create a Feature Matrix and Target Vector\n",
    "target = 'sp_ahead_pos_neg'\n",
    "\n",
    "y = df[target]\n",
    "X = df.drop(columns='sp_ahead_pos_neg')\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(348, 10) (348,)\n(79, 10) (79,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data = create train and test sets \n",
    "# Will use 5-fold cross-validation with our training set\n",
    "\n",
    "mask = df.index < '2014-01-01'\n",
    "\n",
    "X_train, y_train = X.loc[mask], y.loc[mask]\n",
    "X_test, y_test = X.loc[~mask], y.loc[~mask]\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The majority class is True - \"The Market Went Up\"\nOur basline accuracy score is 62.93%\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the distribution of the target variable, 'sp_ahead_pos_neg', and determine our \n",
    "# majority class - the market has been up in the month ahead about 62.93% of the time.\n",
    "# There is no gross overweighting of our classes, and we can use accuracy score to assess.\n",
    "# We will also explore precision/recall and ROC-AUC curves for multiple models. \n",
    "\n",
    "baseline_outcomes = y_train.value_counts(normalize=True)*100\n",
    "print(f'The majority class is True - \"The Market Went Up\"')\n",
    "print(f'Our basline accuracy score is {baseline_outcomes[1]:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "# Logistic Regression \n",
    "model_lr = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "model_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6494252873563219"
      ]
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "model_lr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('randomforestclassifier', RandomForestClassifier())])"
      ]
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "# Random Forest Classifier \n",
    "model_rf = make_pipeline(\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "model_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "source": [
    "model_rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some interesting hyperparameters and tune the model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('DS-Unit-2-Applied-Modeling': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "d622d70ea189f7cb8cf8ccbdaaba9654b73e2442a584c3244680e4f111006bdc"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
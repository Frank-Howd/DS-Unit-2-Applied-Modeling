{
 "cells": [
  {
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 2, Sprint 3, Module 2*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Wrangle ML datasets\n",
    "\n",
    "- [ ] Continue to clean and explore your data. \n",
    "- [ ] For the evaluation metric you chose, what score would you get just by guessing?\n",
    "- [ ] Can you make a fast, first model that beats guessing?\n",
    "\n",
    "**We recommend that you use your portfolio project dataset for all assignments this sprint.**\n",
    "\n",
    "**But if you aren't ready yet, or you want more practice, then use the New York City property sales dataset for today's assignment.** Follow the instructions below, to just keep a subset for the Tribeca neighborhood, and remove outliers or dirty data. [Here's a video walkthrough](https://youtu.be/pPWFw8UtBVg?t=584) you can refer to if you get stuck or want hints!\n",
    "\n",
    "- Data Source: [NYC OpenData: NYC Citywide Rolling Calendar Sales](https://data.cityofnewyork.us/dataset/NYC-Citywide-Rolling-Calendar-Sales/usep-8jbt)\n",
    "- Glossary: [NYC Department of Finance: Rolling Sales Data](https://www1.nyc.gov/site/finance/taxes/property-rolling-sales-data.page)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(439, 11)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            corp_profits  exports_goods_svs  net_exports       gdp  \\\n",
       "1984-01-01       220.108            292.975      -95.004  7483.371   \n",
       "1984-02-01           NaN                NaN          NaN       NaN   \n",
       "1984-03-01           NaN                NaN          NaN       NaN   \n",
       "1984-04-01       220.957            302.200     -104.301  7612.668   \n",
       "1984-05-01           NaN                NaN          NaN       NaN   \n",
       "\n",
       "            10yr_treasury    cpi  industrial_prod  unemployment_rate  \\\n",
       "1984-01-01      11.674286  102.1          54.6008                8.0   \n",
       "1984-02-01      11.842105  102.6          54.8350                7.8   \n",
       "1984-03-01      12.319091  102.9          55.1052                7.8   \n",
       "1984-04-01      12.633500  103.3          55.4514                7.7   \n",
       "1984-05-01      13.408636  103.5          55.7141                7.4   \n",
       "\n",
       "            initial_claims  housing_starts  sp_ahead_pos_neg  \n",
       "1984-01-01        351000.0          1897.0             False  \n",
       "1984-02-01        339250.0          2260.0             False  \n",
       "1984-03-01        345800.0          1663.0             False  \n",
       "1984-04-01        369250.0          1851.0             False  \n",
       "1984-05-01        360250.0          1774.0             False  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>corp_profits</th>\n      <th>exports_goods_svs</th>\n      <th>net_exports</th>\n      <th>gdp</th>\n      <th>10yr_treasury</th>\n      <th>cpi</th>\n      <th>industrial_prod</th>\n      <th>unemployment_rate</th>\n      <th>initial_claims</th>\n      <th>housing_starts</th>\n      <th>sp_ahead_pos_neg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1984-01-01</th>\n      <td>220.108</td>\n      <td>292.975</td>\n      <td>-95.004</td>\n      <td>7483.371</td>\n      <td>11.674286</td>\n      <td>102.1</td>\n      <td>54.6008</td>\n      <td>8.0</td>\n      <td>351000.0</td>\n      <td>1897.0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1984-02-01</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11.842105</td>\n      <td>102.6</td>\n      <td>54.8350</td>\n      <td>7.8</td>\n      <td>339250.0</td>\n      <td>2260.0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1984-03-01</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12.319091</td>\n      <td>102.9</td>\n      <td>55.1052</td>\n      <td>7.8</td>\n      <td>345800.0</td>\n      <td>1663.0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1984-04-01</th>\n      <td>220.957</td>\n      <td>302.200</td>\n      <td>-104.301</td>\n      <td>7612.668</td>\n      <td>12.633500</td>\n      <td>103.3</td>\n      <td>55.4514</td>\n      <td>7.7</td>\n      <td>369250.0</td>\n      <td>1851.0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1984-05-01</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13.408636</td>\n      <td>103.5</td>\n      <td>55.7141</td>\n      <td>7.4</td>\n      <td>360250.0</td>\n      <td>1774.0</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "# Wrangle Data\n",
    "\n",
    "def readIn(file, parse_d='DATE', idx='DATE'):\n",
    "    \"\"\" Opens .csv file, creates datetime index, and returns DataFrame\"\"\"\n",
    "\n",
    "    DATA_PATH = '../data/build_finance/'\n",
    "    df = pd.read_csv(DATA_PATH+file,\n",
    "                     parse_dates=[parse_d]).set_index(idx)\n",
    "    return df\n",
    "\n",
    "\n",
    "def manyToOne(files_m, files_q, file_w):\n",
    "    \"\"\"Accepts lists of .csv files and returns single DataFrame\"\"\"\n",
    "\n",
    "    # Takes monthly and quarterly files, has them read_in, parses their\n",
    "    # dates, and return DataFrames held in lists\n",
    "    frames_m = [readIn(file) for file in files_m]\n",
    "    frames_q = [readIn(file) for file in files_q]\n",
    "\n",
    "    # Reads in SP500 data, indicates columns to use, and capitalizes 'DATE'\n",
    "    # for consistency with other DataFrames held in frames_m and frames_q\n",
    "    DATA_PATH = '../data/build_finance/'\n",
    "    sp = pd.read_csv(DATA_PATH+file_w, usecols=['Date', 'Close'],\n",
    "                     parse_dates=['Date']).set_index('Date')\n",
    "    sp.rename(columns={'Date': 'DATE', 'Close': 'SP500_CLOSE'}, inplace=True)\n",
    "\n",
    "    # Concatenate DataFrames held in frames_m and frames_q\n",
    "    concat_m = pd.concat(frames_m, axis=1)\n",
    "    concat_q = pd.concat(frames_q, axis=1)\n",
    "\n",
    "    # Final concatenation of all DataFrames (monthly data, quarterly data \n",
    "    # and SP500 data)\n",
    "    last = pd.concat([concat_m, concat_q, sp], axis=1)\n",
    "\n",
    "    # SP500 data is only available from 1/1/1985\n",
    "    # Mask out dates prior to January 1st, 1985\n",
    "    mask = last.index >= '1984-01-01'\n",
    "    df_final = last[mask]\n",
    "    df_final.shape\n",
    "\n",
    "    # Return DataFrame\n",
    "    return df_final\n",
    "\n",
    "\n",
    "def wrangle(files_m, files_q, file_w):\n",
    "    # Pass .csv files and have a single DataFrame returned\n",
    "    df = manyToOne(files_m, files_q, file_w)\n",
    "\n",
    "    # Rename columns\n",
    "    df.columns = ['cpi', '10yr_treasury', 'housing_starts', \n",
    "                  'industrial_prod', 'initial_claims', 'unemployment_rate', \n",
    "                  'corp_profits', 'exports_goods_svs', 'gdp', 'net_exports',\n",
    "                  'sp500_close']\n",
    "\n",
    "    # Reorganize columns\n",
    "    cols_reorder = ['corp_profits', 'exports_goods_svs', 'net_exports', \n",
    "                    'gdp', '10yr_treasury', 'cpi', 'industrial_prod', \n",
    "                    'unemployment_rate', 'initial_claims', 'housing_starts', \n",
    "                    'sp500_close']\n",
    "\n",
    "    df = df.reindex(columns=cols_reorder)\n",
    "\n",
    "    # Create a target feature, month-ahead-return: positive/negative - \n",
    "    # Binary Classification\n",
    "    df['sp_ahead_pos_neg'] = (df['sp500_close'].shift(-1) - df['sp500_close']) > 0\n",
    "\n",
    "    # Drop leaky feature\n",
    "    df.drop(columns='sp500_close', inplace=True)\n",
    "\n",
    "    # Create new feature the shows the montly change in monthly initial unemployment \n",
    "    # claims\n",
    "    # df['change_initial_claims'] = df['initial_claims'] / df['initial_claims'].shift(+1)\n",
    "\n",
    "    # # Drop 'initial_claims'\n",
    "    # df.drop(columns='initial_claims')\n",
    "\n",
    "    # Return wrangled DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "file_w = '^GSPC_m.csv'\n",
    "\n",
    "files_m = ['CPI.csv',\n",
    "           'DGS10.csv',\n",
    "           'HOUST.csv',\n",
    "           'INDPRO.csv',\n",
    "           'INITCLMS.csv',\n",
    "           'UNRATE.csv']\n",
    "\n",
    "files_q = ['CP.csv',\n",
    "           'EXPGS.csv',\n",
    "           'GDP.csv',\n",
    "           'NETEXP.csv',]\n",
    "\n",
    "\n",
    "#df = manyToOne(files_m, files_q, file_w)\n",
    "\n",
    "df = wrangle(files_m, files_q, file_w)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True     62.186788\n",
       "False    37.813212\n",
       "Name: sp_ahead_pos_neg, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "# Let's look at the distribution of the target variable, 'sp_ahead_pos_neg', and determine our \n",
    "# majority class - the market has been up in the month ahead about 62.2% of the time.\n",
    "# There is no gross overweighting of our classes, and we can use accuracy score to assess.\n",
    "# We will also explore precision/recall and ROC-AUC curves for multiple models. \n",
    "\n",
    "df['sp_ahead_pos_neg'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('DS-Unit-2-Applied-Modeling': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "d622d70ea189f7cb8cf8ccbdaaba9654b73e2442a584c3244680e4f111006bdc"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}